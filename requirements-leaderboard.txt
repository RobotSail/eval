lm-eval[ifeval,vllm,math,sentencepiece]>=0.4.4

# vLLM v0.9 requires newer CUDA drivers than what is run on most devices
vllm<0.9
torch
